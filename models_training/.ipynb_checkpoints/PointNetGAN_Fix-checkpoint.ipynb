{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3d4444e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import os \n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "558d92f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c6e27a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirt = 'D:/DataSet/shape_net_core_uniform_samples_2048/'\n",
    "folders = os.listdir(dirt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccb735a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "for fold in folders:\n",
    "    direc = dirt + fold\n",
    "    for file in os.listdir(direc):\n",
    "        files.append(direc + '/' + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "251ac00b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57449"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "119ccc1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:/DataSet/shape_net_core_uniform_samples_2048/02691156/10155655850468db78d106ce0a280f87.ply'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7aba444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read .ply file\n",
    "input_file = files[10]\n",
    "pcd = o3d.io.read_point_cloud(input_file) # Read the point cloud\n",
    "\n",
    "# Visualize the point cloud within open3d\n",
    "o3d.visualization.draw_geometries([pcd]) \n",
    "# Convert open3d format to numpy array\n",
    "# Here, you have the point cloud in numpy format. \n",
    "point_cloud_in_numpy = np.asarray(pcd.points) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd1661be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(point_cloud_in_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fb9f5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pts = []\n",
    "        for input_file in tqdm(files):\n",
    "            pcd = o3d.io.read_point_cloud(input_file)\n",
    "            point_cloud_in_numpy = np.asarray(pcd.points) \n",
    "            pts.append(point_cloud_in_numpy)  \n",
    "            \n",
    "        #maxlen = max([len(x) for x in pts])\n",
    "        #pad_pts = np.zeros((len(pts), maxlen, 3))    \n",
    "        self.pts = pts\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pts)\n",
    "    def __getitem__(self, idx):\n",
    "        pts = self.pts[idx]\n",
    "        pts = torch.tensor(pts.T).float().contiguous()\n",
    "        return pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb813b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from PointGAN import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b39c02ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70c28287243943788560e9444ceac8a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57449 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = TrainDataset()\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18a21b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2d9ad34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2048])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58bf61da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7abc3a6ae7ab45bb8bf8b1df3f7613cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Su Lab\\Intern\\PointGAN.py:126: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x), trans\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_27140/2344718011.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0moptimizerG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'[%d: %d/%d] train lossD: %f lossG: %f'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlossD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlossG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number"
     ]
    }
   ],
   "source": [
    "classifier = Discriminator()\n",
    "gen = Generator()\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "classifier.apply(weights_init)\n",
    "gen.apply(weights_init)\n",
    "\n",
    "classifier.cuda()\n",
    "gen.cuda()\n",
    "\n",
    "optimizerD = optim.Adagrad(classifier.parameters(), lr = 0.001)\n",
    "optimizerG = optim.Adagrad(gen.parameters(), lr = 0.001)\n",
    "\n",
    "num_batch = len(dataset)/64\n",
    "\n",
    "for epoch in tqdm(range(3)):\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        optimizerD.zero_grad()\n",
    "        points = data\n",
    "        points = Variable(points)\n",
    "\n",
    "        bs = points.size()[0]\n",
    "        target = Variable(torch.from_numpy(np.ones(bs,).astype(np.int64))).cuda()\n",
    "        #points = points.transpose(2,1)\n",
    "        points = points.cuda()\n",
    "        #print(points.size())\n",
    "\n",
    "        pred, trans = classifier(points)\n",
    "        loss1 = F.nll_loss(pred, target)\n",
    "\n",
    "        sim_noise = Variable(torch.randn(bs, 128)).cuda()\n",
    "        fake = gen(sim_noise)\n",
    "        fake_target = Variable(torch.from_numpy(np.zeros(bs,).astype(np.int64))).cuda()\n",
    "        pred2, trans2 = classifier(fake)\n",
    "\n",
    "        loss2 = F.nll_loss(pred2, fake_target)\n",
    "\n",
    "        lossD = (loss1 + loss2)/2\n",
    "        lossD.backward()\n",
    "        #print(pred, target)\n",
    "        optimizerD.step()\n",
    "\n",
    "        optimizerG.zero_grad()\n",
    "        sim_noise = Variable(torch.randn(bs, 128)).cuda()\n",
    "        points = gen(sim_noise)\n",
    "        pred, trans = classifier(points)\n",
    "        target = Variable(torch.from_numpy(np.ones(bs,).astype(np.int64))).cuda()\n",
    "        #print(pred, target)\n",
    "        lossG = F.nll_loss(pred, target)\n",
    "        lossG.backward()\n",
    "        optimizerG.step()\n",
    "\n",
    "        print('[%d: %d/%d] train lossD: %f lossG: %f' %(epoch, i, num_batch, lossD.data[0], lossG.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa17ded",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b089a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd = generated_pcd[10]\n",
    "points = pcd.permute(1,0).cpu().detach().numpy()\n",
    "pcd  = o3d.geometry.PointCloud()\n",
    "points = o3d.utility.Vector3dVector(points) \n",
    "pcd.points = points\n",
    "# Visualize the point cloud within open3d\n",
    "o3d.visualization.draw_geometries([pcd]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9aa5f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.eval()\n",
    "with torch.no_grad():\n",
    "    z = Variable(Tensor(np.random.normal(0, 1, (32, 128))))\n",
    "    generated_pcd = generator(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e305d480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_chamfer(xyz1, xyz2):\n",
    "    xyz1 = xyz1.permute(0, 2, 1)\n",
    "    xyz2 = xyz2.permute(0, 2, 1)\n",
    "    distance = torch.cdist(xyz1, xyz2)\n",
    "    dist1, idx1 = distance.min(2)\n",
    "    dist2, idx2 = distance.min(1)\n",
    "    loss = (dist1 ** 2).mean() + (dist2 ** 2).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0867a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist1, idx1, dist2, idx2 = chamfer_distance(src.unsqueeze(0), tgt.unsqueeze(0))\n",
    "# loss = dist1.mean() + dist2.mean()\n",
    "loss = (dist1 ** 2).mean() + (dist2 ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80f82310",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = gen\n",
    "Tensor = torch.cuda.FloatTensor\n",
    "generator.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (imgs) in enumerate(train_loader):\n",
    "        real_imgs = Variable(imgs.type(Tensor))\n",
    "        z = Variable(Tensor(np.random.normal(0, 1, (32, 128))))\n",
    "        fake_imgs = generator(z)\n",
    "        break\n",
    "    metric = evaluate_chamfer(real_imgs, fake_imgs)\n",
    "    generator.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b59dd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "xyz1 = real_imgs\n",
    "xyz2 =fake_imgs\n",
    "distance = torch.cdist(xyz1, xyz2)\n",
    "dist1, idx1 = distance.min(2)\n",
    "dist2, idx2 = distance.min(1)\n",
    "loss = (dist1 ** 2).mean() + (dist2 ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7636a4c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c656b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {'D': discriminator.state_dict(), 'D_O': optimizer_D.state_dict(), 'G': generator.state_dict(), 'G_O': optimizer_G.state_dict()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d3128be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_dict, 'PGAN_20.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7b89f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
