{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db627f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c732ab0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import os \n",
    "from tqdm.notebook import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbd318c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_dir = \"D:\\SampleSet\\MVS Data\\Points\\camp\\camp001_l3.ply\" #'D:\\SampleSet\\MVS Data\\Surfaces\\camp\\camp001_l3_surf_11_trim_8.ply'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48887ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = pt_dir\n",
    "pcd = o3d.io.read_point_cloud(input_file) # Read the point cloud\n",
    "# Visualize the point cloud within open3d\n",
    "o3d.visualization.draw_geometries([pcd]) \n",
    "# Convert open3d format to numpy array\n",
    "# Here, you have the point cloud in numpy format. \n",
    "point_cloud_in_numpy = np.asarray(pcd.points) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40799f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23578679, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "point_cloud_in_numpy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4547ab51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(pt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d9d3e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['camp001_l3_surf_11_trim_8.ply', 'camp006_l3_surf_11_trim_8.ply']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(pt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b26bb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "\n",
    "\n",
    "def read_pfm(filename):\n",
    "    file = open(filename, 'rb')\n",
    "    color = None\n",
    "    width = None\n",
    "    height = None\n",
    "    scale = None\n",
    "    endian = None\n",
    "\n",
    "    header = file.readline().decode('utf-8').rstrip()\n",
    "    if header == 'PF':\n",
    "        color = True\n",
    "    elif header == 'Pf':\n",
    "        color = False\n",
    "    else:\n",
    "        raise Exception('Not a PFM file.')\n",
    "\n",
    "    dim_match = re.match(r'^(\\d+)\\s(\\d+)\\s$', file.readline().decode('utf-8'))\n",
    "    if dim_match:\n",
    "        width, height = map(int, dim_match.groups())\n",
    "    else:\n",
    "        raise Exception('Malformed PFM header.')\n",
    "\n",
    "    scale = float(file.readline().rstrip())\n",
    "    if scale < 0:  # little-endian\n",
    "        endian = '<'\n",
    "        scale = -scale\n",
    "    else:\n",
    "        endian = '>'  # big-endian\n",
    "\n",
    "    data = np.fromfile(file, endian + 'f')\n",
    "    shape = (height, width, 3) if color else (height, width)\n",
    "\n",
    "    data = np.reshape(data, shape)\n",
    "    data = np.flipud(data)\n",
    "    file.close()\n",
    "    return data, scale\n",
    "\n",
    "\n",
    "def save_pfm(filename, image, scale=1):\n",
    "    file = open(filename, \"wb\")\n",
    "    color = None\n",
    "\n",
    "    image = np.flipud(image)\n",
    "\n",
    "    if image.dtype.name != 'float32':\n",
    "        raise Exception('Image dtype must be float32.')\n",
    "\n",
    "    if len(image.shape) == 3 and image.shape[2] == 3:  # color image\n",
    "        color = True\n",
    "    elif len(image.shape) == 2 or len(image.shape) == 3 and image.shape[2] == 1:  # greyscale\n",
    "        color = False\n",
    "    else:\n",
    "        raise Exception('Image must have H x W x 3, H x W x 1 or H x W dimensions.')\n",
    "\n",
    "    file.write('PF\\n'.encode('utf-8') if color else 'Pf\\n'.encode('utf-8'))\n",
    "    file.write('{} {}\\n'.format(image.shape[1], image.shape[0]).encode('utf-8'))\n",
    "\n",
    "    endian = image.dtype.byteorder\n",
    "\n",
    "    if endian == '<' or endian == '=' and sys.byteorder == 'little':\n",
    "        scale = -scale\n",
    "\n",
    "    file.write(('%f\\n' % scale).encode('utf-8'))\n",
    "\n",
    "    image.tofile(file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65e77459",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08a14498",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MVSDataset(Dataset):\n",
    "    def __init__(self, datapath, listfile, mode, nviews, ndepths=192, interval_scale=1.06, **kwargs):\n",
    "        super(MVSDataset, self).__init__()\n",
    "        self.datapath = datapath\n",
    "        self.listfile = listfile\n",
    "        self.mode = mode\n",
    "        self.nviews = nviews\n",
    "        self.ndepths = ndepths\n",
    "        self.interval_scale = interval_scale\n",
    "\n",
    "        assert self.mode in [\"train\", \"val\", \"test\"]\n",
    "        self.metas = self.build_list()\n",
    "\n",
    "    def build_list(self):\n",
    "        metas = []\n",
    "        with open(self.listfile) as f:\n",
    "            scans = f.readlines()\n",
    "            scans = [line.rstrip() for line in scans]\n",
    "\n",
    "        # scans\n",
    "        for scan in scans:\n",
    "            pair_file = \"Cameras/pair.txt\"\n",
    "            # read the pair file\n",
    "            with open(os.path.join(self.datapath, pair_file)) as f:\n",
    "                num_viewpoint = int(f.readline())\n",
    "                # viewpoints (49)\n",
    "                for view_idx in range(num_viewpoint):\n",
    "                    ref_view = int(f.readline().rstrip())\n",
    "                    src_views = [int(x) for x in f.readline().rstrip().split()[1::2]]\n",
    "                    # light conditions 0-6\n",
    "                    for light_idx in range(7):\n",
    "                        metas.append((scan, light_idx, ref_view, src_views))\n",
    "        print(\"dataset\", self.mode, \"metas:\", len(metas))\n",
    "        return metas\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metas)\n",
    "\n",
    "    def read_cam_file(self, filename):\n",
    "        with open(filename) as f:\n",
    "            lines = f.readlines()\n",
    "            lines = [line.rstrip() for line in lines]\n",
    "        # extrinsics: line [1,5), 4x4 matrix\n",
    "        extrinsics = np.fromstring(' '.join(lines[1:5]), dtype=np.float32, sep=' ').reshape((4, 4))\n",
    "        # intrinsics: line [7-10), 3x3 matrix\n",
    "        intrinsics = np.fromstring(' '.join(lines[7:10]), dtype=np.float32, sep=' ').reshape((3, 3))\n",
    "        # depth_min & depth_interval: line 11\n",
    "        depth_min = float(lines[11].split()[0])\n",
    "        depth_interval = float(lines[11].split()[1]) * self.interval_scale\n",
    "        return intrinsics, extrinsics, depth_min, depth_interval\n",
    "\n",
    "    def read_img(self, filename):\n",
    "        img = Image.open(filename)\n",
    "        # scale 0~255 to 0~1\n",
    "        np_img = np.array(img, dtype=np.float32) / 255.\n",
    "        return np_img\n",
    "\n",
    "    def read_depth(self, filename):\n",
    "        # read pfm depth file\n",
    "        return np.array(read_pfm(filename)[0], dtype=np.float32)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        meta = self.metas[idx]\n",
    "        scan, light_idx, ref_view, src_views = meta\n",
    "        # use only the reference view and first nviews-1 source views\n",
    "        view_ids = [ref_view] + src_views[:self.nviews - 1]\n",
    "\n",
    "        imgs = []\n",
    "        mask = None\n",
    "        depth = None\n",
    "        depth_values = None\n",
    "        proj_matrices = []\n",
    "\n",
    "        for i, vid in enumerate(view_ids):\n",
    "            # NOTE that the id in image file names is from 1 to 49 (not 0~48)\n",
    "            img_filename = os.path.join(self.datapath,\n",
    "                                        'Rectified/{}_train/rect_{:0>3}_{}_r5000.png'.format(scan, vid + 1, light_idx))\n",
    "            mask_filename = os.path.join(self.datapath, 'Depths/{}_train/depth_visual_{:0>4}.png'.format(scan, vid))\n",
    "            depth_filename = os.path.join(self.datapath, 'Depths/{}_train/depth_map_{:0>4}.pfm'.format(scan, vid))\n",
    "            proj_mat_filename = os.path.join(self.datapath, 'Cameras/train/{:0>8}_cam.txt').format(vid)\n",
    "\n",
    "            imgs.append(self.read_img(img_filename))\n",
    "            intrinsics, extrinsics, depth_min, depth_interval = self.read_cam_file(proj_mat_filename)\n",
    "\n",
    "            # multiply intrinsics and extrinsics to get projection matrix\n",
    "            proj_mat = extrinsics.copy()\n",
    "            proj_mat[:3, :4] = np.matmul(intrinsics, proj_mat[:3, :4])\n",
    "            proj_matrices.append(proj_mat)\n",
    "\n",
    "            if i == 0:  # reference view\n",
    "                depth_values = np.arange(depth_min, depth_interval * self.ndepths + depth_min, depth_interval,\n",
    "                                         dtype=np.float32)\n",
    "                mask = self.read_img(mask_filename)\n",
    "                depth = self.read_depth(depth_filename)\n",
    "\n",
    "        imgs = np.stack(imgs).transpose([0, 3, 1, 2])\n",
    "        proj_matrices = np.stack(proj_matrices)\n",
    "\n",
    "        return {\"imgs\": imgs,\n",
    "                \"proj_matrices\": proj_matrices,\n",
    "                \"depth\": depth,\n",
    "                \"depth_values\": depth_values,\n",
    "                \"mask\": mask}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2245de4",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../lists/dtu/train.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19476/2000514230.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m dataset = MVSDataset(\"/home/xyguo/dataset/dtu_mvs/processed/mvs_training/dtu/\", '../lists/dtu/train.txt', 'train',\n\u001b[1;32m----> 2\u001b[1;33m                          3, 128)\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19476/1219084310.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, datapath, listfile, mode, nviews, ndepths, interval_scale, **kwargs)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"train\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"val\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"test\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbuild_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19476/1219084310.py\u001b[0m in \u001b[0;36mbuild_list\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbuild_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mmetas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistfile\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m             \u001b[0mscans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0mscans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mscans\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../lists/dtu/train.txt'"
     ]
    }
   ],
   "source": [
    "dataset = MVSDataset(\"D:\\SampleSet\\MVS Data\\\", '../lists/dtu/train.txt', 'train',\n",
    "                         3, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38364abc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
